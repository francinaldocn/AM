{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando Regressão Múltipla do Zero\n",
    "\n",
    "Nessa tarefa você vai estender sua implementação da tarefa passada para considerar múltiplas variáveis. Você pode estender a versão vetorizada implementada [neste notebook](https://canvas.instructure.com/courses/1389733/files/68372962/download?verifier=jx3MnZn2ltc955iEt69PwjkzJvBiuQyUNXwGA9gl&wrap=1) para regressão simples. \n",
    "\n",
    "- Rode o algoritmo [nesses dados](https://canvas.instructure.com/courses/1389733/files/68372968/download?verifier=9Qm2NF62mBNlbfeLJWfrzjGl6qvS0eIuX3kOGNAU&wrap=1), onde as linhas representam as notas de alunos de computação de alunos da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação.\n",
    "\n",
    "- Compare o valor dos coeficientes estimados pelo seu algoritmo com o valor dos coeficientes da regressão linear do scikit learn para testar se o seu algoritmo está funcionando corretamente.\n",
    "\n",
    "A entrega deve ser o link no seu github para o notebook Jupyter com código python e texto explicativo quando necessário. De preferência, crie um repositório na sua conta do github e envie o link do html do notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versão vetorizada para Regressão Linear Simples\n",
    "\n",
    "A versão vetorizada de regressão linear simples servirá de base para esternder a implementação de uma versão vetorizada da regressão linear múltipla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))\n",
    "    \n",
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:,1][:,np.newaxis]\n",
    "    m_gradient = np.sum(np.multiply(res,X))\n",
    "    new_w = np.array([(w_current[0] + (2 * learningRate * b_gradient)),\n",
    "             (w_current[1] + (2 * learningRate * m_gradient))])\n",
    "    return [new_w,b_gradient,m_gradient]\n",
    "\n",
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w,b_gradient,m_gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        grad = np.array([b_gradient,m_gradient])\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w\n",
    "\n",
    "points = np.genfromtxt(\"income.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,[0,1]]\n",
    "Y = points[:,2][:,np.newaxis]\n",
    "init_w = np.zeros((2,1))\n",
    "learning_rate = 0.0001\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, error = {2}\".format(init_w[0], init_w[1], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, error = {2}\".format(w[0], w[1], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versão vetorizada, estendida para Regressão Linear Múltipla (RLMV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return (totalError / float(len(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current, X, y, learning_rate):\n",
    "    m = len(y)\n",
    "    res = Y - np.dot(X, w_current)\n",
    "    gradient = -2 * (X.T @ res)\n",
    "    new_w = w_current - (learning_rate/m) * X.T @ (X @ w_current - y)\n",
    "    return [new_w, gradient]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X, y, learning_rate, epsilon, iterations):\n",
    "    w = starting_w\n",
    "    grad = np.linalg.norm(X)\n",
    "    iter = 1\n",
    "    #while (np.linalg.norm(grad) >= epsilon):\n",
    "    for iter in range(iterations):\n",
    "        w, gradient = step_gradient_vectorized(w,X,Y,learning_rate)\n",
    "        grad = gradient\n",
    "        if iter % 100000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(iter,compute_mse_vectorized(w, X, Y)))\n",
    "            iter+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rode o algoritmo [nesses dados](https://canvas.instructure.com/courses/1389733/files/68372968/download?verifier=9Qm2NF62mBNlbfeLJWfrzjGl6qvS0eIuX3kOGNAU&wrap=1), onde as linhas representam as notas de alunos de computação de alunos da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w0 = [0.], w1 = [0.],  w2 = [0.],  w3 = [0.],  w4 = [0.],  w5 = [0.], error = [[54.47995386]]\n",
      "\n",
      "Running...\n",
      "gradient : 1.780077889634732e-09\n",
      "\n",
      "O gradiente descendente convergiu com w0 = [1.73771151], w1 = [0.10304143], w2 = [0.0464367],  w3 = [0.16409834],  w4 = [0.38117843],  w5 = [0.02027816], error = [[0.41133759]]\n",
      "\n",
      "A versão vetorizada rodou em: 20593.058586120605 ms\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset\n",
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "# Remover a primeira linha da matriz que contém o nome das colunas\n",
    "points = np.delete(points, 0,0)\n",
    "# Adicionar uma coluna extra de 1, para w0\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "# Selecionando os atributos no dataset (variáres explicativas/independentes)\n",
    "X = points[:, :-1]\n",
    "# Selecionando a variável explicada/dependente\n",
    "Y = points[:,-1][:,np.newaxis]\n",
    "# Iniciando w com zeros\n",
    "init_w = np.zeros(((len(points[0,:])-1),1))\n",
    "# Taxa de aprendizado\n",
    "learning_rate = 0.003 \n",
    "# Número de iterações\n",
    "iterations = 1000000\n",
    "# \n",
    "epsilon = 1.7801187720365064e-09\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1},  w2 = {2},  w3 = {3},  w4 = {4},  w5 = {5}, error = {6}\".format(init_w[0], init_w[1], init_w[2], init_w[3], init_w[4], init_w[5], compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"\\nRunning...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X, Y, learning_rate, epsilon, iterations)\n",
    "toc = time.time()\n",
    "print(\"\\nO gradiente descendente convergiu com w0 = {0}, w1 = {1}, w2 = {2},  w3 = {3},  w4 = {4},  w5 = {5}, error = {6}\".format(w[0], w[1], w[2], w[3], w[4], w[5], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"\\nA versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação da Regressão Linear Múltipa utilizando a biblioteca scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação utilizando o scikit será feita apenas para obenção dos coeficientes com o intuido de compará-los com os obtidos na implementação da Regressão Liner Múltipla vetorizada.\n",
    "\n",
    "Para esse fim serão utilizadas apenas as bibliotecas numpy e sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importando as biblioteca numpy e sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset fornecido no notebook\n",
    "points = np.genfromtxt('sample_treino.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributos - Variáveis independentes\n",
    "X = points[1:, :-1]\n",
    "# Objetivo - Variável dependente\n",
    "y = points[1:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary least squares Linear Regression.\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit linear model\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes Estimados\n",
      "w0 : 1.73771151\n",
      "w1 : 0.10304143\n",
      "w2 : 0.04643670\n",
      "w3 : 0.16409834\n",
      "w4 : 0.38117843\n",
      "w5 : 0.02027816\n"
     ]
    }
   ],
   "source": [
    "# Mostrando os coeficientes estimados\n",
    "print('Coeficientes Estimados')\n",
    "print('w0 : {:.8f}'.format(lm.intercept_))\n",
    "atributo = np.array(['w1','w2','w3','w4','w5'])\n",
    "for i in range(0,5):\n",
    "    print(atributo[i],\": {:.8f}\".format(lm.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando o valor dos coeficientes estimados pelo algoritmo implementando (RMLV) e SCIKIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tabela 01 - Comparativo dos coeficientes obtidos nas duas abordagens, RMLV e SCIKIT\n",
    " \n",
    "    |  Coef  |    RLMV    |   SCIKIT   |\n",
    "    |--------|------------|------------|\n",
    "    |   w0   | 1.73771151 | 1.73771151 |\n",
    "    |   w1   | 0.10304143 | 0.10304143 |\n",
    "    |   w2   | 0.0464367  | 0.04643670 |\n",
    "    |   w3   | 0.16409834 | 0.16409834 |\n",
    "    |   w4   | 0.38117843 | 0.38117842 |\n",
    "    |   w5   | 0.02027816 | 0.02027815 |\n",
    "    \n",
    "Pela Tabela 01, verificamos que os coeficientes obtidos da implementação da regressão linear múltipla vetorizada - RLMV, obtivemos os mesmos valores do scikit learn. \n",
    "\n",
    "Desse modo, verifica-se que a o algoritmo implementado está funcionando corretamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    " $\\hat{y}$ = 1.73771151 + 0.10304143$\\cdot$**Cálculo1** + 0.04643670$\\cdot$**LPT** + 0.16409834$\\cdot$**P1** + 0.38117843$\\cdot$**IC** + 0.02027816$\\cdot$**Cálculo2** + $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretando os coeficientes:\n",
    "\n",
    "- Mantendo todas as outras variáveis constantes, um aumento de 1 unidade em **Cálculo1** está associado a um **aumento de 0.10304143** no CRA.\n",
    "- Mantendo todas as outras variáveis constantes, um aumento de 1 unidade em **LPT** está associada a um **aumento de 0.04643670** no CRA.\n",
    "- Mantendo todas as outras variáveis constantes, um aumento de 1 unidade em **P1** está associada a um **aumento de 0.16409834** no CRA.\n",
    "- Mantendo todas as outras variáveis constantes, um aumento de 1 unidade em **IC** está associada a um **aumento de 0.38117843** no CRA.\n",
    "- Mantendo todas as outras variáveis constantes, um aumento de 1 unidade em **Cálculo2** está associado a um **aumento de 0.02027816** no CRA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
